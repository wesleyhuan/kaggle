{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91723,"databundleVersionId":14272474,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/wesleyhuan/improve-solution-season-5-episode-12?scriptVersionId=288554634\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\nThis notebook is directly copy from my last work.\n\nLink : [https://www.kaggle.com/code/wesleyhuan/easy-and-simple-solution-season-5-episode-12](http://)\n\nThis notebook will skip EDA and data visualization part. You can check it at the link above.\n\nI ran into the problem that AUC is not imporving too much after hyperparameter(Optuna) and K-fold wasting too much time.\n\nSo I come up with 2 types of solution.\n\n1. increase feature from correlation matrix(done)\n2. OOF(Out-of-Fold)(done)","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, Pool\nimport optuna\nfrom sklearn.preprocessing import OrdinalEncoder,LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n# config\n#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n#print(device)\nclass CFG:\n    train_csv = '/kaggle/input/playground-series-s5e12/train.csv'\n    test_csv = '/kaggle/input/playground-series-s5e12/test.csv'\n    sample_submission_csv = '/kaggle/input/playground-series-s5e12/sample_submission.csv'\n    N_FOLDS = 5\n    RANDOM_SEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:38:32.382315Z","iopub.execute_input":"2025-12-26T14:38:32.382959Z","iopub.status.idle":"2025-12-26T14:38:42.97142Z","shell.execute_reply.started":"2025-12-26T14:38:32.382924Z","shell.execute_reply":"2025-12-26T14:38:42.970456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(CFG.train_csv)\ntest = pd.read_csv(CFG.test_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:38:42.972332Z","iopub.execute_input":"2025-12-26T14:38:42.97337Z","iopub.status.idle":"2025-12-26T14:38:46.906906Z","shell.execute_reply.started":"2025-12-26T14:38:42.973344Z","shell.execute_reply":"2025-12-26T14:38:46.906082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess and clean data\n\n**Why use a Class?**\n\nState Retention: You need to remember the median values and the Encoders from the Training set so you can apply exactly the same rules to the Test set.\n\n\nAvoids Leakage: Your previous function calculated the median of the test set using the test set. In a real scenario, you must use the training set's median to fill the test set.\n\n\nHandles Unknowns: I replaced LabelEncoder with OrdinalEncoder. LabelEncoder crashes if the Test set has a category not seen in Train. OrdinalEncoder can be configured to handle these safely.","metadata":{}},{"cell_type":"code","source":"class DiabetesPreprocessor:\n    def __init__(self):\n        self.medians = {}\n        self.encoders = {}\n        self.numeric_cols = []\n        self.categorical_cols = []\n        \n    def fit(self, df):\n        \"\"\"\n        Learn the parameters (medians, categories) from the TRAINING data.\n        \"\"\"\n        # Identify columns\n        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        # 1. Learn Medians for numeric columns\n        for col in self.numeric_cols:\n            self.medians[col] = df[col].median()\n            \n        # 2. Fit Encoders for categorical columns\n        # handle_unknown='use_encoded_value' prevents crashes if Test data has new categories\n        for col in self.categorical_cols:\n            enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n            # We must reshape to (-1, 1) for sklearn encoders\n            enc.fit(df[[col]].astype(str)) \n            self.encoders[col] = enc\n            \n        return self\n\n    def transform(self, df):\n        \"\"\"\n        Apply the learned parameters to the data (Train or Test).\n        \"\"\"\n        df = df.copy()\n        \n        # 1. Drop irrelevant columns (ID is usually dropped, Target handled separately)\n        # Note: We don't drop target here to keep X and y aligned until the end\n        if 'id' in df.columns:\n            df = df.drop(columns=['id'])\n            \n        # 2. Impute Missing Values using LEARNED medians\n        for col in self.numeric_cols:\n            if col in df.columns:\n                df[col] = df[col].fillna(self.medians.get(col, 0))\n        \n        # 3. Apply Encoding\n        for col in self.categorical_cols:\n            if col in df.columns:\n                # Fill NaN in categoricals with 'Missing' before encoding to be safe\n                df[col] = df[col].astype(str).fillna('Missing')\n                df[col] = self.encoders[col].transform(df[[col]])\n        \n        return df\n        \n    def create_interaction_features(self, df):# add because of the Correlation matrix\n        df = df.copy()\n        \n        # 1. Pulse Pressure (Heart/Artery stress)\n        # Higher pulse pressure is linked to diabetes complications\n        if 'systolic_bp' in df.columns and 'diastolic_bp' in df.columns:\n            df['pulse_pressure'] = df['systolic_bp'] - df['diastolic_bp']\n            \n        # 2. Cholesterol Ratios (Metabolic health)\n        # Avoid division by zero by adding a tiny epsilon if needed, \n        # though HDL is rarely 0 in real data.\n        if 'cholesterol_total' in df.columns and 'hdl_cholesterol' in df.columns:\n            df['cholesterol_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-5)\n            \n        if 'ldl_cholesterol' in df.columns and 'hdl_cholesterol' in df.columns:\n            df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1e-5)\n    \n        # 3. Visceral Fat Proxy (The \"Bad\" Fat)\n        # Combining general obesity (BMI) with central obesity (Waist)\n        if 'bmi' in df.columns and 'waist_to_hip_ratio' in df.columns:\n            df['visceral_fat_index'] = df['bmi'] * df['waist_to_hip_ratio']\n            \n        # 4. Activity-to-Screen Ratio\n        # Measures sedentary lifestyle balance\n        if 'physical_activity_minutes_per_week' in df.columns and 'screen_time_hours_per_day' in df.columns:\n            # Convert screen time to minutes to make units comparable\n            screen_mins = df['screen_time_hours_per_day'] * 60\n            df['activity_screen_ratio'] = df['physical_activity_minutes_per_week'] / (screen_mins + 1)\n        # Interaction: Age and systolic_bp have 0.5 correlation \n        if 'age' in df.columns and 'systolic_bp' in df.columns:\n            df['age_systolic_bp_interaction'] = df['age'] * df['systolic_bp']\n        return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:38:46.909221Z","iopub.execute_input":"2025-12-26T14:38:46.909591Z","iopub.status.idle":"2025-12-26T14:38:46.922304Z","shell.execute_reply.started":"2025-12-26T14:38:46.909538Z","shell.execute_reply":"2025-12-26T14:38:46.921136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the preprocessor\npreprocessor = DiabetesPreprocessor()\n\n# Separate Target from Train for fitting (optional, but cleaner)\n# It is best to calculate stats on the features, not including the target\nX_train_raw = train.drop(columns=['diagnosed_diabetes'])\ny_train = train['diagnosed_diabetes']\ntest_ids = test['id']\n\n# FIT on Training Data Only (Learn the rules)\npreprocessor.fit(X_train_raw)\n\n# TRANSFORM both Train and Test (Apply the rules)\nX_train_processed = preprocessor.transform(X_train_raw)\nX_test_processed = preprocessor.transform(test)\n\nX_train_add_feature = preprocessor.create_interaction_features(X_train_processed)\nX_test_add_feature = preprocessor.create_interaction_features(X_test_processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:38:46.923435Z","iopub.execute_input":"2025-12-26T14:38:46.924202Z","iopub.status.idle":"2025-12-26T14:38:49.864017Z","shell.execute_reply.started":"2025-12-26T14:38:46.924176Z","shell.execute_reply":"2025-12-26T14:38:49.863247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Calculate the correlation matrix\n# We use the processed data so categorical columns (encoded as numbers) are included\ncorr_matrix = X_train_processed.corr()\n\n# 2. Setup the figure size (make it large enough to read)\nplt.figure(figsize=(14, 12))\n\n# 3. Create the Heatmap\n# annot=True: shows the numbers\n# cmap='coolwarm': Blue for negative corr, Red for positive\n# fmt=\".2f\": limits decimals to 2 places\nsns.heatmap(\n    corr_matrix, \n    annot=True, \n    fmt=\".2f\", \n    cmap='coolwarm', \n    center=0, \n    vmin=-1, \n    vmax=1,\n    linewidths=0.5\n)\n\nplt.title(\"Feature Correlation Matrix\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:38:49.865057Z","iopub.execute_input":"2025-12-26T14:38:49.865406Z","iopub.status.idle":"2025-12-26T14:38:52.6668Z","shell.execute_reply.started":"2025-12-26T14:38:49.865381Z","shell.execute_reply":"2025-12-26T14:38:52.665318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Strategy 1: The \"Difference\" Feature (Blood Pressure)\n\nWhat to look for: Look at **systolic_bp** and **diastolic_bp**. \n\nObservation: They will likely be highly correlated (red square) because if one is high, the other is usually high. In this data it seems like they don't. Instead of **systolic_bp** and **diastolic_bp** **age** seems have more  relation.\n\nThe New Feature: The gap between them is medically significant. It's called Pulse Pressure. A widening gap indicates stiffening arteries, which is a diabetes risk factor.\n\nStrategy 2: The \"Ratio\" Feature (Cholesterol)\n\nWhat to look for: Look at **ldl_cholesterol**, **hdl_cholesterol**, and **cholesterol_total**. \n\nObservation: They might have strong correlations. The New Feature: In medicine, the balance between \"bad\" (LDL) and \"good\" (HDL) cholesterol is a better predictor of heart/metabolic health than just the total number.\n\nStrategy 3: The \"Interaction\" Feature (BMI & Waist)\n\nWhat to look for: **bmi** and **waist_to_hip_ratio**. \n\nObservation: Both measure obesity, so they will be correlated. The New Feature: A person can have a high BMI but be muscular (low risk). However, if they have high BMI AND a high Waist-to-Hip ratio, that indicates Visceral Fat (fat around organs), which is the #1 driver of Type 2 Diabetes.","metadata":{}},{"cell_type":"markdown","source":"\n# Train model with Hyper parameter (OPTUNA)\n","metadata":{}},{"cell_type":"code","source":"X_tuning, _, y_tuning, _ = train_test_split(\n    X_train_add_feature, y_train, \n    train_size=0.4, # Tune on 40% of data\n    stratify=y_train, \n    random_state=CFG.RANDOM_SEED\n)\n\n# Define the number of folds\ndef objective_cv(trial, X, y, n_folds=CFG.N_FOLDS, random_seed=CFG.RANDOM_SEED):\n    \"\"\"\n    Optuna objective function that uses Stratified K-Fold Cross-Validation.\n    \"\"\"\n    # 1. Define Hyperparameters using Optuna trial suggestions\n    param = {\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"use_label_encoder\": False,\n        \"tree_method\": 'hist', # Faster training method\n        \"booster\": 'gbtree',\n        \"random_state\": random_seed,\n        \n        # Hyperparameters to tune\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True), # Use log scale for LR\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000, step=50),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 8),\n        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n    }\n\n    # 2. Setup Stratified K-Fold\n    # Ensure stable splits regardless of data size\n    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n    auc_scores = []\n    \n    # 3. Training Loop (Cross-Validation)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Instantiate and train model\n        model = xgb.XGBClassifier(**param)\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            verbose=False\n        )\n\n        # Predict probabilities and calculate AUC for this fold\n        y_pred_prob = model.predict_proba(X_val)[:, 1]\n        auc = roc_auc_score(y_val, y_pred_prob)\n        auc_scores.append(auc)\n        \n    # 4. Return the mean AUC across all folds\n    mean_auc = np.mean(auc_scores)\n    #Print the result for the current trial\n    print(f\"Trial {trial.number:3d} finished with mean CV AUC: {mean_auc:.6f}\")\n    \n    return mean_auc\n\n\ntrain_x = X_train_add_feature \ntrain_y = y_train \n# Build and run Optuna study\nprint(\"Starting Optuna study with Stratified K-Fold...\")\nstudy = optuna.create_study(direction=\"maximize\")  # Maximize the mean AUC\n# Pass the full training data (X and y) to the objective function\nstudy.optimize(lambda trial: objective_cv(trial, X_tuning, y_tuning), n_trials=20)\nprint(\"Study complete.\")\n\n# Result\nprint(\"\\nüéâ Best parameters found by CV Optuna:\")\nprint(study.best_params)\nprint(f\"Best Mean CV AUC: {study.best_value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T14:38:52.668154Z","iopub.execute_input":"2025-12-26T14:38:52.668621Z","iopub.status.idle":"2025-12-26T15:16:40.044583Z","shell.execute_reply.started":"2025-12-26T14:38:52.668589Z","shell.execute_reply":"2025-12-26T15:16:40.043599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Âª∫Ë≠∞‰øÆÊîπÁöÑÊúÄÂæåË®ìÁ∑¥ÈöéÊÆµ\ndef train_and_predict(X, y, X_test, params):\n    kf = StratifiedKFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.RANDOM_SEED)\n    oof_preds = np.zeros(len(X))\n    test_preds = np.zeros(len(X_test))\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        model = xgb.XGBClassifier(**params, tree_method='hist', random_state=CFG.RANDOM_SEED)\n        \n        # Â¢ûÂä†Êó©ÂÅúÊ©üÂà∂ (Early Stopping) Èò≤Ê≠¢ÈÅéÊì¨Âêà\n        model.fit(X_tr, y_tr, \n                  eval_set=[(X_val, y_val)], \n                  verbose=False)\n        \n        # È†êÊ∏¨È©óË≠âÈõÜËàáÊ∏¨Ë©¶ÈõÜ\n        oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n        test_preds += model.predict_proba(X_test)[:, 1] / CFG.N_FOLDS\n        \n        print(f\"Fold {fold+1} AUC: {roc_auc_score(y_val, oof_preds[val_idx]):.6f}\")\n        \n    print(f\"Overall OOF AUC: {roc_auc_score(y, oof_preds):.6f}\")\n    return test_preds\n\n# ‰ΩøÁî® Optuna ÊâæÂà∞ÁöÑÊúÄ‰Ω≥ÂèÉÊï∏Âü∑Ë°å\nfinal_test_preds = train_and_predict(train_x, train_y, X_test_add_feature, study.best_params)\n\n# Êõ¥Êñ∞Êèê‰∫§\nsubmission = pd.DataFrame({\n    'id': test['id'],  # Use the original IDs from the test set\n    'diagnosed_diabetes': final_test_preds\n})\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"\\nSubmission file created: cv_optuna_submission.csv\")\n\nprint(f\"Submission file created: {submission.shape}\")\nprint(\"First 5 rows of submission:\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T15:16:40.045678Z","iopub.execute_input":"2025-12-26T15:16:40.046029Z","iopub.status.idle":"2025-12-26T15:22:18.847718Z","shell.execute_reply.started":"2025-12-26T15:16:40.046Z","shell.execute_reply":"2025-12-26T15:22:18.846664Z"}},"outputs":[],"execution_count":null}]}